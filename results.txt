options: 

+===============================================================================================+
| TRT-LL3.1-8b-OCSG  | 5.51       | 593.24         | 6050.88        | 6553.19        | OK       |
| TensorRT-20b-OCSG2 | 2.84       | 332.32         | 11573.91       | 12439.11       | OK       |
| TensorRT-20b-OCSG@ | 3.43       | 401.28         | 9109.75        | 10688.64       | OK       |
| TensorRT-120b-OCSG | 1.44       | 167.73         | 23131.95       | 24887.23       | OK       |
| vLLM-GLM-Intellect | 0.08       | 9.62           | 60031.44       | 60061.53       | OK       |
+===============================================================================================+

./benchmark.sh --all --text-output ./bench1.txt
./benchmark.sh --model-script ./models/llama31_8b_trt_speculative.sh --text-output bench10.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-11 20:22:38                                                             |
+===============================================================================================+
| Model: nvidia/Llama-3.1-8B-Instruct-FP8                                                       |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM       | 2.97       | 324.86         | 9590.68        | 12008.73       | OK       |
| SGLang             | 3.79       | 416.67         | 8325.90        | 11837.85       | OK       |
| vLLM               | 4.28       | 480.30         | 7838.73        | 9577.84        | OK       |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM-OCSG  | 5.51       | 593.24         | 6050.88        | 6553.19        | OK       |
+-----------------------------------------------------------------------------------------------+

+===============================================================================================+
| Benchmark-SMI 2025-12-11 20:54:10                                                             |
+===============================================================================================+
| Model: nvidia/Llama-3.1-8B-Instruct-FP8                                                       |
| Prompts: 20  Concurrency: 8  Max output tokens: 128  Temp: 0.0                                |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM       | 1.35       | 151.03         | 4938.89        | 5469.96        | OK       |
| SGLang             | 1.16       | 131.65         | 5550.36        | 6277.52        | OK       |
| vLLM               | 1.31       | 148.45         | 4911.37        | 5502.87        | OK       |
+-----------------------------------------------------------------------------------------------+

./benchmark.sh --model-scripts --text-output bench2.txt

MODEL_ID=nvidia/Qwen3-30B-A3B-FP4 ./benchmark.sh --all --text-output ./bench3.txt

▶ Running TensorRT-LLM at http://127.0.0.1:8355 ...
  ✓ TensorRT-LLM: 1.34 req/s, output 149.29 tok/s, p50 4999.2 ms, p99 5526.6 ms
▶ Running SGLang at http://127.0.0.1:8002 ...
  ✓ SGLang: 1.15 req/s, output 130.85 tok/s, p50 5585.5 ms, p99 6351.1 ms  

MODEL_ID=ibm-granite/granite-4.0-h-small ./benchmark.sh --all --text-output ./bench4.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-12 17:21:54                                                             |
+===============================================================================================+
| Model: openai/gpt-oss-120b                                                                    |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM       | 1.44       | 167.73         | 23131.95       | 24887.23       | OK       |
+-----------------------------------------------------------------------------------------------+

./benchmark.sh --model-script ./models/intellect3_fp8_vllm.sh --text-output bench13.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-12 22:06:47                                                             |
+===============================================================================================+
| Model: PrimeIntellect/INTELLECT-3-FP8                                                         |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| vLLM               | 0.08       | 9.62           | 60031.44       | 60061.53       | OK       |
+-----------------------------------------------------------------------------------------------+

MODEL_ID=openai/gpt-oss-20b ./benchmark.sh --all --text-output ./bench8.txt
./benchmark.sh --model-script ./models/gpt20b_mxfp4_trt.sh --text-output bench10.txt
./benchmark.sh --model-script ./models/gpt20b_mxfp4_trt_speculative.sh --text-output bench10.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-12 00:18:53                                                             |
+===============================================================================================+
| Model: openai/gpt-oss-20b                                                                     |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM       | 1.30       | 152.18         | 22691.13       | 26411.37       | OK       |
| SGLang             | 1.15       | 143.16         | 31958.95       | 34477.98       | OK       |
| vLLM               | 3.18       | 393.53         | 10184.01       | 11684.63       | OK       |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM-OC    | 2.90       | 339.33         | 10879.61       | 12490.88       | OK       |
| TensorRT-LLM-OCSG  | 2.76       | 324.78         | 11758.44       | 12742.29       | OK       |
| TensorRT-LLM-OCSG@ | 3.43       | 401.28         | 9109.75        | 10688.64       | OK       |
+-----------------------------------------------------------------------------------------------+

MODEL_ID=openai/gpt-oss-120b ./benchmark.sh --all --text-output ./bench9.txt
./benchmark.sh --model-script ./models/gpt120b_mxfp4_trt.sh --text-output bench11.txt
./benchmark.sh --model-script ./models/gpt120b_mxfp4_trt_speculative.sh --text-output bench14.txt
./benchmark.sh --model-script ./models/gpt120b_mxfp4_trt_speculative_dp.sh --text-output bench14.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-12 01:22:55                                                             |
+===============================================================================================+
| Model: openai/gpt-oss-120b                                                                    |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM       | 0.43       | 48.54          | 56200.65       | 60060.94       | OK       |
| SGLang             | 0.27       | 32.07          | 60006.00       | 60061.50       | OK       |
| vLLM               | 1.40       | 175.13         | 22161.05       | 25210.92       | OK       |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM-OCSG  | 1.44       | 167.73         | 23131.95       | 24887.23       | OK       |
| TensorRT-LLM-OCSGD | 1.29       | 150.16         | 23521.56       | 27848.52       | OK       |
+-----------------------------------------------------------------------------------------------+

./benchmark.sh --model-script ./models/llama33_70b_fp4_trt.sh --text-output bench11.txt
./benchmark.sh --model-script ./models/llama33_70b_trt_speculative.sh --text-output bench11.txt
./benchmark.sh --model-script ./models/nemotronsuper49b_trt.sh --text-output bench11.txt

+===============================================================================================+
| Benchmark-SMI 2025-12-12 13:59:46                                                             |
+===============================================================================================+
| Model: meta-llama/Llama-3.3-70B-Instruct                                                      |
| Prompts: 100  Concurrency: 32  Max output tokens: 128  Temp: 0.0                              |
+-----------------------------------------------------------------------------------------------+
| Backend            | Req/s      | Out tok/s      | p50 ms         | p99 ms         | Status   |
+-----------------------------------------------------------------------------------------------+
| TensorRT-LLM-OC     | 1.05       | 119.55         | 29629.38       | 41469.94       | OK       |
| TensorRT-LLM-OCSG   | 0.98       | 110.65         | 32803.83       | 37768.82       | OK       |
+-----------------------------------------------------------------------------------------------+

