diff --git a/vllm/model_executor/layers/batch_invariant.py b/vllm/model_executor/layers/batch_invariant.py
index 4f31e5a..2ef348b 100644
--- a/vllm/model_executor/layers/batch_invariant.py
+++ b/vllm/model_executor/layers/batch_invariant.py
@@ -1056,7 +1056,4 @@ def init_batch_invariance():
         override_envs_for_invariance()
         enable_batch_invariant_mode()
 
-        # Disable TF32 for batch invariance - it causes non-deterministic rounding
-        torch.backends.cuda.matmul.fp32_precision = "ieee"
-        torch.backends.cudnn.conv.fp32_precision = "ieee"
-        torch.backends.cudnn.rnn.fp32_precision = "ieee"
+        # NOTE: TF32 / FP32 precision is configured globally (see vllm/v1/worker/gpu_worker.py).
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 21a8564..b27e1a6 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -82,7 +82,17 @@ class Worker(WorkerBase):
 
         # configure float32 matmul precision according to vLLM env.
         precision = envs.VLLM_FLOAT32_MATMUL_PRECISION
-        torch.backends.cuda.matmul.fp32_precision = precision
+        use_tf32 = precision.lower() == "tf32"
+
+        # Avoid mixing legacy and new TF32 APIs. Use the new matmul precision API.
+        # - "tf32" => enable TF32 fast path for FP32 matmul
+        # - "ieee" => prefer full FP32 precision
+        torch.set_float32_matmul_precision("high" if use_tf32 else "highest")
+
+        # TF32 for cuDNN (conv / RNN) when enabled.
+        torch.backends.cudnn.allow_tf32 = use_tf32
+        torch.backends.cudnn.conv.fp32_precision = "tf32" if use_tf32 else "ieee"
+        torch.backends.cudnn.rnn.fp32_precision = "tf32" if use_tf32 else "ieee"
 
         if self.model_config.trust_remote_code:
             # note: lazy import to avoid importing torch before initializing
